apiVersion: flowctl/v1
kind: Pipeline
metadata:
  name: dag-pipeline
  namespace: analytics
  labels:
    topology: dag
    pattern: fan-out-fan-in
  annotations:
    description: "DAG topology example with parallel processing paths"

spec:
  description: "Demonstrates DAG processing with fan-out to multiple processors and fan-in aggregation"
  driver: docker
  
  sources:
    - id: ledger-source
      type: source
      image: alpine:latest
      command: ["sh", "-c"]
      args: |
        counter=1
        while true; do
          echo "{
            \"ledger_seq\": $counter,
            \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\",
            \"operations\": [
              {\"type\": \"payment\", \"amount\": $((RANDOM % 1000)), \"asset\": \"XLM\"},
              {\"type\": \"create_account\", \"starting_balance\": 100},
              {\"type\": \"path_payment\", \"amount\": $((RANDOM % 500)), \"path\": [\"USD\", \"EUR\", \"XLM\"]}
            ],
            \"fee\": 100,
            \"success\": true
          }"
          counter=$((counter + 1))
          sleep 3
        done
      env:
        NETWORK: "public"
  
  processors:
    # Fan-out: Three parallel processors handle different aspects
    - id: payment-extractor
      type: processor
      image: alpine:latest
      command: ["sh", "-c"]
      args: |
        while read ledger; do
          # Extract payment operations
          echo "$ledger" | sed 's/.*/{\"processor\": \"payment\", \"extracted\": \"payment_ops\", \"ledger\": &}/'
        done
      inputs: ["ledger-source"]
      env:
        PROCESSOR_TYPE: "payment"
    
    - id: account-extractor
      type: processor
      image: alpine:latest
      command: ["sh", "-c"]
      args: |
        while read ledger; do
          # Extract account operations
          echo "$ledger" | sed 's/.*/{\"processor\": \"account\", \"extracted\": \"account_ops\", \"ledger\": &}/'
        done
      inputs: ["ledger-source"]
      env:
        PROCESSOR_TYPE: "account"
    
    - id: path-payment-extractor
      type: processor
      image: alpine:latest
      command: ["sh", "-c"]
      args: |
        while read ledger; do
          # Extract path payment operations
          echo "$ledger" | sed 's/.*/{\"processor\": \"path_payment\", \"extracted\": \"path_ops\", \"ledger\": &}/'
        done
      inputs: ["ledger-source"]
      env:
        PROCESSOR_TYPE: "path_payment"
    
    # Fan-in: Aggregator combines results from all extractors
    - id: aggregator
      type: processor
      image: alpine:latest
      command: ["sh", "-c"]
      args: |
        # Simple aggregator that collects from all inputs
        while true; do
          # In a real implementation, this would properly merge streams
          while read -t 1 data; do
            echo "{\"aggregated\": true, \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\", \"data\": $data}"
          done
          sleep 0.1
        done
      inputs: 
        - "payment-extractor"
        - "account-extractor" 
        - "path-payment-extractor"
      env:
        AGGREGATION_WINDOW: "10s"
        MERGE_STRATEGY: "combine"
  
  sinks:
    - id: analytics-sink
      type: sink
      image: alpine:latest
      command: ["sh", "-c"]
      args: |
        while read data; do
          echo "[ANALYTICS] Processed ledger data: $data"
          # In production, this would write to analytics database
        done
      inputs: ["aggregator"]
      env:
        DESTINATION: "analytics_db"
        BATCH_MODE: "true"